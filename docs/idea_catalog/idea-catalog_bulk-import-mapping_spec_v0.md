# Idea Catalog Bulk Import Mapping Spec v0

> **Status:** Implementation-ready spec  
> **Audience:** internal devs  
> **Scope:** import curated “global catalog” Ideas from bulk files into Supabase

---

## 1) Purpose

Define a deterministic, safe import pipeline that converts **validated** bulk Idea rows into the Rekindle database:

- Upserts `ideas` records (base editorial fields)
- Writes trait links into `idea_traits` using canonical `trait_types` + `trait_options`
- Produces an idempotent import that can be re-run safely for iterative catalog building

This spec assumes the upstream **Linter + Validator** has already been run and either:
- blocks the import if errors exist, or
- produces a normalized JSON payload for the importer.

---

## 2) Source of truth for schema

- Supabase-generated TS types (`lib/database/types.gen.ts`) are the single schema source of truth.
- Import code must be type-safe and treat incoming file data as untrusted boundary input.

---

## 3) Target tables and key fields

### 3.1 `ideas`
Primary “Idea” entity.

Key columns (v0 import-relevant):
- `id` (uuid; generated by DB if omitted)
- `slug` (string; required; treat as immutable once published)
- `title` (string; required)
- `reason_snippet` (string; optional in DB but required by publish gate)
- `description` (string; optional in DB but required by publish gate)
- `min_minutes` / `max_minutes` (numbers; optional in DB but required by publish gate)
- `image_url` (optional)
- `is_global` (boolean; **true** for catalog ideas)
- `is_deleted` (boolean; false for active ideas; true to soft-remove)
- `created_by_user_id` (null for global curated ideas)
- `effort_id` (nullable; see section 7.2)

### 3.2 `trait_types` and `trait_options`
Canonical vocabulary.
- `trait_types.slug` identifies the trait dimension (e.g. `cost_tier`)
- `trait_options.slug` identifies a value option (e.g. `free`)
- Options may be deprecated via `is_deprecated`

### 3.3 `idea_traits`
Join table between ideas and trait options.

Columns:
- `idea_id`
- `trait_type_id`
- `trait_option_id`
- `trait_select_mode` (`"single"` or `"multi"`)

---

## 4) Import pipeline overview

### 4.1 Stages
1) **Read input**: CSV or normalized JSON
2) **Validate (hard gate)**: run linter; abort on any errors
3) **Build registry cache**:
   - trait type slug → id
   - trait option (type slug, option slug) → id
4) **Row import (transactional)**:
   - Upsert idea by `slug`
   - Sync traits (delete + insert per trait type)
5) **Post-import checks**:
   - count imported
   - count traits inserted
   - sample search query verification (manual at first)

### 4.2 Idempotency rule
Re-running import with the same file should converge to the same DB state.

---

## 5) Column mapping (bulk file → database)

This section assumes the canonical CSV schema (template v1).

### 5.1 Base editorial fields → `ideas`

| CSV column | DB column | Notes |
|---|---|---|
| `title` | `ideas.title` | required |
| `reason_snippet` | `ideas.reason_snippet` | required by publish gate |
| `description` | `ideas.description` | required by publish gate |
| `min_minutes` | `ideas.min_minutes` | required by publish gate |
| `max_minutes` | `ideas.max_minutes` | required by publish gate |
| `active` | `ideas.is_deleted` | map `active=false` → `is_deleted=true` |
| *(generated)* | `ideas.slug` | see section 6 |
| `image_url` *(optional future column)* | `ideas.image_url` | optional |

### 5.2 Non-schema editorial columns (ignored by DB in v0)
These columns are stored only in import logs for now:
- `status`, `editorial_note`, `source`

If you want permanence later, we can add:
- `idea_admin_notes` table or `ideas.meta` JSON column (future migration)

### 5.3 Longform fields not present in DB schema (v0 strategy)
The CSV supports:
- `steps`
- `what_you_need`
- `tips_or_variations`
- `safety_or_boundaries_note`

But the current schema does not have dedicated columns/tables for these.

**v0 recommended behavior**:
- Preserve them by **appending** to `ideas.description` using a structured format.

Example (markdown-ish):

```text
<description>

Steps:
- Step 1…
- Step 2…

What you need:
- Item A…
- Item B…

Tips / variations:
- Tip A…

Safety / boundaries:
- Note…
```

Importer should do this transformation only if those columns are present and non-empty.
(Alternatively, store “description as-is” and drop the extras; but preserving is better.)

---

## 6) Slug generation and immutability

### 6.1 Canonical slug
- `ideas.slug` is required in DB.
- Slugs are treated as **immutable once published**.

### 6.2 Generation algorithm (v0)
If the bulk file does not include a slug column, the importer generates one:

- lower-case
- replace non-alphanumeric with `-`
- collapse repeated `-`
- trim `-` ends
- max length 60
- if collision: suffix `-2`, `-3`, etc (warn)

**Important:** once a slug is created and imported, future updates must use the same slug (do not regenerate).

### 6.3 Upsert key
Use `slug` as the stable upsert key for global catalog ideas.

---

## 7) Trait mapping → `idea_traits`

### 7.1 Trait type mapping (CSV column → trait type slug)
Use the same mapping as the linter.

| CSV column | Trait type slug | Select mode | Tier |
|---|---|---:|---:|
| `time_bucket_slug` | `time_bucket` | single | 1 |
| `effort_slug` | `effort` | single | 1 |
| `cost_tier_slug` | `cost_tier` | single | 1 |
| `coordination_level_slug` | `coordination_level` | single | 1 |
| `presence_requirement_slug` | `presence_requirement` | single | 1 |
| `idea_format_slugs` | `idea_format` | multi | 1 |
| `goal_slugs` | `goal` | multi | 1 |
| `context_slugs` | `context` | multi | 1 |
| `idea_category_slugs` | `idea_category` | multi | 1 |
| `relationship_type_fit_slugs` | `person_type` | multi | 1 |
| `event_tag_slugs` | `event_tag` | multi | 2 |
| `idea_collection_slugs` | `idea_collection` | multi | 2 |
| `surprise_style_slug` | `surprise_style` | single | 2 |
| `energy_vibe_slug` | `energy_vibe` | single | 2 |
| `social_setting_slugs` | `social_setting` | multi | 2 |
| `age_band_fit_slugs` | `age_band` | multi | 2 |
| `physical_intensity_slug` | `physical_intensity` | single | 3 |
| `accessibility_flag_slugs` | `accessibility_flag` | multi | 3 |
| `weather_dependence_slugs` | `weather_dependence` | multi | 3 |
| `gender_fit_slugs` | `person_gender` | multi | 3 |

### 7.2 Optional “denormalized” pointers on `ideas`
The `ideas` table includes an `effort_id` column.

**v0 recommendation**
- Always write effort to `idea_traits` (canonical)
- Also set `ideas.effort_id` to the `trait_option_id` for `effort` for compatibility and convenience

If later you add other denormalized pointers (like `time_bucket_id`), follow the same pattern:
- traits remain canonical
- pointers are “cached” and can be recomputed

### 7.3 Trait resolution algorithm
For each trait assignment:

1) Lookup `trait_type_id` via `trait_types.slug = <trait type slug>`
2) Lookup `trait_option_id` via:
   - `trait_options.trait_type_id = trait_type_id`
   - `trait_options.slug = <option slug>`
3) Fail import if:
   - missing trait_type
   - missing option
   - option is deprecated

### 7.4 Trait sync behavior (update strategy)
When importing a row for an existing idea:

- For each trait type included in the import row:
  1) Delete existing `idea_traits` rows for `(idea_id, trait_type_id)`
  2) Insert the new trait rows

This ensures:
- idempotency
- no drift between file and DB
- multi-select changes are applied cleanly

---

## 8) Transaction boundaries and error strategy

### 8.1 Transaction per row (recommended v0)
Each row import runs in its own transaction:

- begin
- upsert idea by slug
- sync traits
- commit

If one row fails:
- log failure with row index + slug/title
- continue importing other rows (optional mode), OR stop immediately (strict mode)

### 8.2 Strict vs permissive modes
- **Strict mode (default)**: any row error stops the entire import
- **Permissive mode**: import valid rows, output a “failed rows” report

---

## 9) Handling “active” / soft deletion

The file includes `active` boolean.

**Mapping**
- `active = true` → `ideas.is_deleted = false`
- `active = false` → `ideas.is_deleted = true`

This makes it safe to “unpublish” ideas without losing history.

---

## 10) Local dev loop integration

The import pipeline must support the repo’s existing local data loop:

1) Update bulk file
2) Run linter (must pass)
3) Run local seed/import
4) Launch app and test search behavior

(Implementation detail: integrate importer into the Supabase seed scripts or a dedicated `npm` script that targets local Supabase.)

---

## 11) Acceptance criteria (definition of done)

Importer is “done” when:

1) It can import 100+ ideas from a clean CSV without manual DB edits.
2) Re-running the import produces no duplicates and cleanly updates traits (idempotent).
3) `search_ideas_v2` returns imported ideas and includes the expected `trait_map` signals.
4) It supports soft-unpublishing via `active=false`.

---

## 12) Future extensions

- Insert `ideas_translations` for locale `'en'` (or others) as part of the same transaction.
- Store `editorial_note` / `source` in an `idea_admin_notes` table.
- Bulk performance optimizations: batch inserts via temporary tables.
- Candidate inbox promotion pipeline (from `custom_requests` and user-submitted ideas).
